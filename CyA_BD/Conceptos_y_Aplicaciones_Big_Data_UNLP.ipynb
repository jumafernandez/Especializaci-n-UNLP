{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conceptos_y_Aplicaciones_Big_Data_UNLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/UNLP/blob/master/CyA_BD/Conceptos_y_Aplicaciones_Big_Data_UNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWF9WA6bY8Gq",
        "colab_type": "text"
      },
      "source": [
        "## **TRABAJO INTEGRADOR: CONCEPTOS Y APLICACIONES DE BIG DATA**\n",
        "\n",
        "__Carrera:__ Espcialización en Inteligencia de Datos orientada a Big Data <br/>\n",
        "__Docente:__ Waldo Hasperué  <br/>\n",
        "__Estudiante:__ Juan Manuel Fernandez\n",
        "<br/>  <br/>\n",
        "__Resolución de Consultas utilizando PySpark:__ <br />\n",
        "\n",
        "_Consulta 1:_ Puntaje histórico promedio de cada uno de los arqueros que jugaron en algún equipo de Uruguay. <br />\n",
        "\n",
        "_Consulta 2:_ ¿Cuál es el nombre del equipo que salio campeón en la temporada 2015 de la primera división de Francia? __(optativo)__ <br />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCAItzJbaacX",
        "colab_type": "text"
      },
      "source": [
        "### Preparación del entorno Pyspark y carga de datos\n",
        "\n",
        "En primer lugar, cargamos los datos de fantasía desde un repositorio Github:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpq_LqF6fVro",
        "colab_type": "code",
        "outputId": "222af4fd-5d38-4f7c-a017-30f6b6783e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/liga.csv\n",
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/equipo.csv\n",
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/jugador.csv\n",
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/encuentro.csv\n",
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/participacion_jugador.csv\n",
        "!wget https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/goles.csv"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-30 22:46:37--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/liga.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 310 [text/plain]\n",
            "Saving to: ‘liga.csv.3’\n",
            "\n",
            "\rliga.csv.3            0%[                    ]       0  --.-KB/s               \rliga.csv.3          100%[===================>]     310  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:37 (94.7 MB/s) - ‘liga.csv.3’ saved [310/310]\n",
            "\n",
            "--2019-09-30 22:46:39--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/equipo.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358 [text/plain]\n",
            "Saving to: ‘equipo.csv.3’\n",
            "\n",
            "equipo.csv.3        100%[===================>]     358  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:39 (62.7 MB/s) - ‘equipo.csv.3’ saved [358/358]\n",
            "\n",
            "--2019-09-30 22:46:41--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/jugador.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 896 [text/plain]\n",
            "Saving to: ‘jugador.csv.3’\n",
            "\n",
            "jugador.csv.3       100%[===================>]     896  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:41 (142 MB/s) - ‘jugador.csv.3’ saved [896/896]\n",
            "\n",
            "--2019-09-30 22:46:42--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/encuentro.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 229 [text/plain]\n",
            "Saving to: ‘encuentro.csv.3’\n",
            "\n",
            "encuentro.csv.3     100%[===================>]     229  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:42 (59.0 MB/s) - ‘encuentro.csv.3’ saved [229/229]\n",
            "\n",
            "--2019-09-30 22:46:43--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/participacion_jugador.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 848 [text/plain]\n",
            "Saving to: ‘participacion_jugador.csv.3’\n",
            "\n",
            "participacion_jugad 100%[===================>]     848  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:43 (203 MB/s) - ‘participacion_jugador.csv.3’ saved [848/848]\n",
            "\n",
            "--2019-09-30 22:46:45--  https://raw.githubusercontent.com/jumafernandez/UNLP/master/CyA_BD/data/goles.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118 [text/plain]\n",
            "Saving to: ‘goles.csv.3’\n",
            "\n",
            "goles.csv.3         100%[===================>]     118  --.-KB/s    in 0s      \n",
            "\n",
            "2019-09-30 22:46:45 (21.9 MB/s) - ‘goles.csv.3’ saved [118/118]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt82LZmqpoGm",
        "colab_type": "text"
      },
      "source": [
        "Instalamos la librería pyspark, Java 8 y seteo las variables de entorno para que no devuelva error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YJgvLUJprSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrVSZif_Wj95",
        "colab_type": "text"
      },
      "source": [
        "Elijo la versión de 8 de Java:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmJ_8QxOo8A",
        "colab_type": "code",
        "outputId": "af611e4b-acf0-48d3-f21f-f90f144f28db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!update-alternatives --config java"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "* 2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5jrFE2boltE",
        "colab_type": "text"
      },
      "source": [
        "Ahora, generamos el entorno PySpark cargando las librerías a utilizar, instanciamos el Spark Context y se da formato columnar a los RDD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc5Ht_OPbA6p",
        "colab_type": "code",
        "outputId": "a2805713-ccf9-4c9f-d792-f71784eaa3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importo las librerias\n",
        "from pyspark import SparkContext, SQLContext\n",
        "\n",
        "# Genero el Spark Context y el SQLContext para la DB \n",
        "sc = SparkContext(\"local\", \"Simple App\") \n",
        "\n",
        "# Cargo SQLContext, solo para instanciar dataframes, no para consultas\n",
        "df = SQLContext(sc)\n",
        "\n",
        "# Defino el path donde estan los archivos (Solo para máquina local) \n",
        "data_path = ''\n",
        "\n",
        "# Cargo los RDD con los datos de los archivos y acomodo los datos\n",
        "RDD_liga = sc.textFile(data_path + \"liga.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id\"').\\\n",
        "           map(lambda d: (int(d[0]), int(d[1]), d[2].replace('\"', ''), d[3].replace('\"', '')))\n",
        "RDD_liga.first()\n",
        "\n",
        "RDD_equipo = sc.textFile(data_path + \"equipo.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id\"').\\\n",
        "           map(lambda d: (int(d[0]), d[1].replace('\"', ''), d[2].replace('\"', ''), d[3].replace('\"', ''), d[4].replace('\"', '')))\n",
        "RDD_equipo.first()\n",
        "\n",
        "RDD_jugador = sc.textFile(data_path + \"jugador.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id\"').\\\n",
        "           map(lambda d: (int(d[0]), d[1].replace('\"', ''), int(d[2]), d[3].replace('\"', '')))\n",
        "RDD_jugador.first()\n",
        "\n",
        "RDD_encuentro = sc.textFile(data_path + \"encuentro.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id\"').\\\n",
        "           map(lambda d: (int(d[0]), int(d[1]), d[2].replace('\"', ''), int(d[3]), int(d[4]), d[5].replace('\"', ''), float(d[6]), int(d[7])))\n",
        "RDD_encuentro.first()\n",
        "\n",
        "RDD_participacion_jugador = sc.textFile(data_path + \"participacion_jugador.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id_jugador\"').\\\n",
        "           map(lambda d: (int(d[0]), int(d[1]), int(d[2]), int(d[3]), int(d[4]), int(d[5]), int(d[6])))\n",
        "RDD_participacion_jugador.first()\n",
        "\n",
        "RDD_goles = sc.textFile(data_path + \"goles.csv\").\\\n",
        "           map(lambda line: line.split(\";\")).\\\n",
        "           filter(lambda d: d[0] != '\"id\"').\\\n",
        "           map(lambda d: (int(d[0]), int(d[1]), int(d[2]),d[3].replace('\"', ''), int(d[4])))\n",
        "\n",
        "\n",
        "RDD_goles.first()\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 2, 'CABEZA', 90)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHMxIRIjcFG4",
        "colab_type": "text"
      },
      "source": [
        "Luego, los transformo a dataframes por dos cuestiones centrales:\n",
        "1. _Conflictos con el Join en los RDD:_ Cuando intento aplicar un join sobre un RDD puro, me proyecta solo un subconjuto de los atributos.\n",
        "2. _Comodidad:_ trabajando con dataframes, puedo referenciar los datos por nombre de columnas, lo cual brinda mayor expresividad y facilidad para la resolución de las consultas.\n",
        "\n",
        "Independientemente de esto, no se utilizarán las otras características de SparkSQL (consultas SQL) en la resolución del trabajo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEoYEZSYcc8o",
        "colab_type": "code",
        "outputId": "a26d3f48-ba25-499a-dd78-9e87ed831e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Creo los dataframes\n",
        "df_ligas = df.createDataFrame(RDD_liga,['id','temporada', 'division', 'pais'])\n",
        "df_equipos =  df.createDataFrame(RDD_equipo,['id','nombre', 'pais', 'ciudad', 'fundacion'])\n",
        "df_jugadores = df.createDataFrame(RDD_jugador,['id','nombre', 'edad', 'posicion'])\n",
        "df_encuentros = df.createDataFrame(RDD_encuentro,['id','id_liga', 'fecha', 'id_equipo_local', 'id_equipo_visitante', 'estadio', 'recaudacion', 'localidades_vendidas'])\n",
        "df_participaciones = df.createDataFrame(RDD_participacion_jugador,['id_jugador','id_equipo', 'id_partido', 'puntaje_recibido', 'cantidad_amarillas', 'cantidad_rojas', 'minutos_jugados'])\n",
        "df_goles = df.createDataFrame(RDD_goles,['id', 'id_jugador', 'id_partido', 'tipo_gol', 'minuto_gol'])\n",
        "\n",
        "df_encuentros.show()\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+----------+---------------+-------------------+------------------+-----------+--------------------+\n",
            "| id|id_liga|     fecha|id_equipo_local|id_equipo_visitante|           estadio|recaudacion|localidades_vendidas|\n",
            "+---+-------+----------+---------------+-------------------+------------------+-----------+--------------------+\n",
            "|  2|      1|2019-05-07|              1|                  2|ALBERTO J. ARMANDO|  2500000.0|               60000|\n",
            "|  3|      1|2019-10-01|              2|                  1|  VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "+---+-------+----------+---------------+-------------------+------------------+-----------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UYAN8IKbaVj",
        "colab_type": "text"
      },
      "source": [
        "### Resolución Consulta 1: Puntaje histórico promedio de cada uno de los arqueros que jugaron en algún equipo de Uruguay.\n",
        "\n",
        "Para comenzar con la resolución, definimos parámetros para flexibilizar la consulta (posición y país):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaBY1qfdapeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posicion = 'ARQUERO'\n",
        "pais_filtro = 'ARGENTINA'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPgpfcm3cy24",
        "colab_type": "text"
      },
      "source": [
        "Tomo los equipos de la variable _pais_filtro_ y los jugadores que juegan en el contenido de la variable _posicion_:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTfK2wHuc_lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equipos_pais = df_equipos.filter(df_equipos.pais==pais_filtro)\n",
        "jugadores_posicion = df_jugadores.filter(df_jugadores.posicion==posicion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHriFac1dF24",
        "colab_type": "text"
      },
      "source": [
        "A continuación, resuelvo el resto de la consulta tomando las participaciones de los arqueros en equipos uruguayos y agrupando las instancias aplicando la función de agregación MEAN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjLqDv2AdF_W",
        "colab_type": "code",
        "outputId": "d2191f4c-9648-447b-c200-a8bb7e88abfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Joineo ambas cosas\n",
        "participaciones = jugadores_posicion.join(df_participaciones, jugadores_posicion.id==df_participaciones.id_jugador)\n",
        "\n",
        "# Tomo las columnas que me interesan y muestro el resultado agrupando\n",
        "jugadores_puntaje = participaciones.select('nombre', 'puntaje_recibido')\n",
        "jugadores_puntaje.groupby('nombre').agg({'puntaje_recibido': 'mean'}).show()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+---------------------+\n",
            "|          nombre|avg(puntaje_recibido)|\n",
            "+----------------+---------------------+\n",
            "|  ARMANI, FRANCO|                  4.0|\n",
            "|ANDRADA, ESTEBAN|                  8.0|\n",
            "+----------------+---------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnGIIK8pexC",
        "colab_type": "text"
      },
      "source": [
        "### Resolución Consulta ¿Cuál es el nombre del equipo que salio campeón en la temporada 2015 de la primera división de Francia?\n",
        "\n",
        "Para comenzar con la resolución, definimos parámetros para flexibilizar la consulta (temporada, división y país):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ifiZCMpwHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parametros de búsqueda\n",
        "temporada = 2019\n",
        "division = 'PRIMERA A'\n",
        "pais = 'ARGENTINA'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFoQW7lpz6H",
        "colab_type": "text"
      },
      "source": [
        "En primer instancia, recupero el _id_ de la liga que corresponde a la temporada, division y pais de la consulta:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNlPxVFmp8rB",
        "colab_type": "code",
        "outputId": "d75185e4-75db-4cdb-f56d-84b86596acad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "RDD_liga_q2 = RDD_liga.filter(lambda d: d[3] == pais).\\\n",
        "                      filter(lambda d: d[1] == temporada).\\\n",
        "                      filter(lambda d: d[2] == division)\n",
        "\n",
        "id_liga_buscada = RDD_liga_q2.first()[0]\n",
        "\n",
        "print('La liga buscada tiene id ' + str(id_liga_buscada))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La liga buscada tiene id 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdXpPvGhqBBI",
        "colab_type": "text"
      },
      "source": [
        "A continuación, selecciono todas las participaciones de jugadores en encuentros con el id de la liga recuperada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jem9DWEuqLCk",
        "colab_type": "code",
        "outputId": "47df6766-0d53-4673-c6d0-3e37772a8cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "encuentros_liga = df_encuentros.filter(df_encuentros.id_liga==id_liga_buscada)\n",
        "participaciones_liga = df_participaciones.join(encuentros_liga, df_participaciones.id_partido==encuentros_liga.id, how='left')\n",
        "participaciones_liga = participaciones_liga.filter(participaciones_liga.id_liga==id_liga_buscada)\n",
        "\n",
        "# Muestro los datos recuperados\n",
        "participaciones_liga.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+----------+----------------+------------------+--------------+---------------+---+-------+----------+---------------+-------------------+----------------+-----------+--------------------+\n",
            "|id_jugador|id_equipo|id_partido|puntaje_recibido|cantidad_amarillas|cantidad_rojas|minutos_jugados| id|id_liga|     fecha|id_equipo_local|id_equipo_visitante|         estadio|recaudacion|localidades_vendidas|\n",
            "+----------+---------+----------+----------------+------------------+--------------+---------------+---+-------+----------+---------------+-------------------+----------------+-----------+--------------------+\n",
            "|         1|        1|         3|               8|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         2|        1|         3|               6|                 2|             1|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         3|        1|         3|               7|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         4|        1|         3|               8|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         5|        1|         3|               5|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         6|        1|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         7|        1|         3|               9|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         8|        1|         3|              10|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|         9|        1|         3|               8|                 1|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        10|        1|         3|               6|                 0|             1|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        11|        1|         3|               6|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        12|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        13|        2|         3|               4|                 2|             1|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        14|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        15|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        16|        2|         3|               5|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        17|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        18|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        19|        2|         3|               4|                 0|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "|        20|        2|         3|               8|                 1|             0|             90|  3|      1|2019-10-01|              2|                  1|VESPUCIO LIBERTI|  3000000.0|               40000|\n",
            "+----------+---------+----------+----------------+------------------+--------------+---------------+---+-------+----------+---------------+-------------------+----------------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jztI_iAzqPoz",
        "colab_type": "text"
      },
      "source": [
        "Ahora _joineo_ las participaciones de los jugadores con los goles que es lo que me interesa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAOyo7G2qb3a",
        "colab_type": "code",
        "outputId": "2e1c6720-871d-4f96-99f2-190800b0217d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Primero, cambio el nombre de id_partido por un problema Python con columnas repetidas\n",
        "goles = df_goles.selectExpr('id_jugador', 'id_partido as id_partido_gol')\n",
        "\n",
        "# Realizo el join\n",
        "goles_por_participaciones=goles.join(participaciones_liga, (goles.id_jugador==participaciones_liga.id_jugador) & (goles.id_partido_gol==participaciones_liga.id_partido))\n",
        "\n",
        "# Me quedo solo con los atributos que necesito\n",
        "goles_por_equipo_partido = goles_por_participaciones.selectExpr('id_equipo', 'id_partido')\n",
        "\n",
        "goles_por_equipo_partido.show()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|id_equipo|id_partido|\n",
            "+---------+----------+\n",
            "|        1|         3|\n",
            "|        1|         2|\n",
            "|        1|         2|\n",
            "+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdgrVul3qlzj",
        "colab_type": "text"
      },
      "source": [
        "Acto seguido, agrupo los goles por equipo y partido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbRhhYvLq-8g",
        "colab_type": "code",
        "outputId": "f69a6adb-f86b-4915-ce2c-e3238d45c7ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "goles_por_equipo_partido = goles_por_equipo_partido.groupBy('id_equipo', 'id_partido').count()\n",
        "\n",
        "goles_por_equipo_partido.show()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+----------+-----+\n",
            "|id_equipo|id_partido|count|\n",
            "+---------+----------+-----+\n",
            "|        1|         3|    1|\n",
            "|        1|         2|    2|\n",
            "+---------+----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60kpdM1OrXua",
        "colab_type": "text"
      },
      "source": [
        "Ahora que ya tengo los goles por participación, voy a joinear los datos con los partidos que se jugaron en la liga consultada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQcEMDPErWwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Me quedo con las columnas que necesito\n",
        "encuentros = encuentros_liga.selectExpr('id', 'id_equipo_local', 'id_equipo_visitante') # Me quedo solo con los atributos que necesito\n",
        "\n",
        "# Joineo con id_equipo_local\n",
        "encuentros = encuentros.join(goles_por_equipo_partido, (encuentros.id==goles_por_equipo_partido.id_partido) & (encuentros.id_equipo_local==goles_por_equipo_partido.id_equipo), how=\"left\")\n",
        "encuentros = encuentros.withColumnRenamed('count', 'goles_equipo_local')\n",
        "encuentros = encuentros.selectExpr('id', 'id_equipo_local', 'id_equipo_visitante', 'goles_equipo_local') # Me quedo solo con los atributos que necesito\n",
        "\n",
        "# Joineo con id_equipo_visitante\n",
        "encuentros = encuentros.join(goles_por_equipo_partido, (encuentros.id==goles_por_equipo_partido.id_partido) & (encuentros.id_equipo_visitante==goles_por_equipo_partido.id_equipo), how=\"left\")\n",
        "encuentros = encuentros.withColumnRenamed('count', 'goles_equipo_visitante')\n",
        "\n",
        "# Me quedo unicamente con los id de partido, equipos y con los goles por equipo\n",
        "encuentros = encuentros.selectExpr('id', 'id_equipo_local', 'id_equipo_visitante', 'goles_equipo_local', 'goles_equipo_visitante') # Me quedo solo con los atributos que necesito\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYfBLBxOrmGi",
        "colab_type": "text"
      },
      "source": [
        "Me traigo algunas funciones pyspark para manipular los _dataframe_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hby9Lgp4rv1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import when, lit, desc\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OfSlqBqrx6a",
        "colab_type": "text"
      },
      "source": [
        "A continuación, ya que tengo los resultados de cada partido, le asigno puntos a cada resultado. Para ello, tengo que generar dos atributos nuevos, uno denominado __puntos_equipo_local__ y otro __puntos_equipo_visitante__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7LJEiEosGqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Antes, para que funcione, reemplazo los None de los equipos que no hicieron goles en los partidos por 0\n",
        "df_resultados = encuentros.withColumn('goles_equipo_local', when(encuentros.goles_equipo_local.isNull(), \n",
        "lit(0)).otherwise(encuentros.goles_equipo_local)).withColumn('goles_equipo_visitante', when(encuentros.goles_equipo_visitante.isNull(), \n",
        "lit(0)).otherwise(encuentros.goles_equipo_visitante))\n",
        "\n",
        "# Ahora veo cuantos puntos se quedaron por partido equipos locales y visitantes\n",
        "df_resultados = df_resultados.withColumn('puntos_equipo_local',\n",
        "                                        F.when((F.col(\"goles_equipo_local\") > F.col(\"goles_equipo_visitante\")), 3)\\\n",
        "                                        .when((F.col(\"goles_equipo_local\") == F.col(\"goles_equipo_visitante\")), 1)\\\n",
        "                                        .otherwise(0))\n",
        "\n",
        "df_resultados = df_resultados.withColumn('puntos_equipo_visitante',\n",
        "                                        F.when((F.col(\"goles_equipo_visitante\") > F.col(\"goles_equipo_local\")), 3)\\\n",
        "                                        .when((F.col(\"goles_equipo_visitante\") == F.col(\"goles_equipo_local\")), 1)\\\n",
        "                                        .otherwise(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6cvAjy4sMCG",
        "colab_type": "text"
      },
      "source": [
        "Realizo una operación de union de id_equipo_local y id_equipo_visitante y aplano la lista, todo para luego armar la tabla de posiciones de la liga:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiUM1jE9sbxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elijo solo los atributos que necesito\n",
        "df_puntos_local = df_resultados.selectExpr('id', 'id_equipo_local as id_equipo', 'puntos_equipo_local as puntos')\n",
        "df_puntos_visitante = df_resultados.selectExpr('id', 'id_equipo_visitante as id_equipo', 'puntos_equipo_visitante as puntos')\n",
        "\n",
        "# Union\n",
        "df_puntaje_partido = df_puntos_local.union(df_puntos_visitante)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pxsKydjsi5a",
        "colab_type": "text"
      },
      "source": [
        "Genero la tabla de posiciones, para ello agrupo puntos por equipo y luego joineo la tabla con el dataframe _equipos_ a efectos de traerme los nombres de los mismos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKjQuL5vszYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agrupo por id_equipo, sumo los puntos obtenidos y ordeno (esta es la tabla de posiciones)\n",
        "tabla_posiciones = df_puntaje_partido.groupby('id_equipo').agg({'puntos': 'sum'})\\\n",
        "                   .sort(desc('sum(puntos)')).withColumnRenamed('sum(puntos)', 'Puntos')\n",
        "\n",
        "# Hago un join con el df equipos para traer el nombre de los equipos\n",
        "tabla_posiciones=tabla_posiciones.join(df_equipos, tabla_posiciones.id_equipo==df_equipos.id)\n",
        "tabla_posiciones=tabla_posiciones.selectExpr('nombre as Equipo', 'Puntos')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyfo4GUUs54t",
        "colab_type": "text"
      },
      "source": [
        "Muestro la Tabla de Posiciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlwOYhwFs8CC",
        "colab_type": "code",
        "outputId": "d4b35f77-b365-4703-d6f8-ed05784e152f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tabla_posiciones.show()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-c46a39c06d18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtabla_posiciones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3776.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(id_equipo#3087L, 200)\n+- *(55) Project [id_equipo#3087L, sum(puntos)#3103L AS Puntos#3107L]\n   +- *(55) Sort [sum(puntos)#3103L DESC NULLS LAST], true, 0\n      +- Exchange rangepartitioning(sum(puntos)#3103L DESC NULLS LAST, 200)\n         +- *(54) HashAggregate(keys=[id_equipo#3087L], functions=[sum(cast(puntos#3088 as bigint))], output=[id_equipo#3087L, sum(puntos)#3103L])\n            +- Exchange hashpartitioning(id_equipo#3087L, 200)\n               +- *(53) HashAggregate(keys=[id_equipo#3087L], functions=[partial_sum(cast(puntos#3088 as bigint))], output=[id_equipo#3087L, sum#3141L])\n                  +- Union\n                     :- *(26) Project [id_equipo_local#2558L AS id_equipo#3087L, CASE WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END > CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 3 WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END = CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 1 ELSE 0 END AS puntos#3088]\n                     :  +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n                     :     :- *(14) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n                     :     :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n                     :     :     +- *(13) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n                     :     :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n                     :     :           :- *(2) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n                     :     :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n                     :     :           :     +- *(1) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n                     :     :           :        +- *(1) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_local#2558L))\n                     :     :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n                     :     :           +- *(12) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n                     :     :              +- *(12) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n                     :     :                 +- Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n                     :     :                    +- *(11) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[partial_count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2843L])\n                     :     :                       +- *(11) Project [id_equipo#2572L, id_partido#2573L]\n                     :     :                          +- *(11) SortMergeJoin [id_jugador#2586L, id_partido_gol#2763L], [id_jugador#2571L, id_partido#2573L], Inner\n                     :     :                             :- *(4) Sort [id_jugador#2586L ASC NULLS FIRST, id_partido_gol#2763L ASC NULLS FIRST], false, 0\n                     :     :                             :  +- Exchange hashpartitioning(id_jugador#2586L, id_partido_gol#2763L, 200)\n                     :     :                             :     +- *(3) Project [id_jugador#2586L, id_partido#2587L AS id_partido_gol#2763L]\n                     :     :                             :        +- *(3) Filter (isnotnull(id_partido#2587L) && isnotnull(id_jugador#2586L))\n                     :     :                             :           +- Scan ExistingRDD[id#2585L,id_jugador#2586L,id_partido#2587L,tipo_gol#2588,minuto_gol#2589L]\n                     :     :                             +- *(10) Sort [id_jugador#2571L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n                     :     :                                +- Exchange hashpartitioning(id_jugador#2571L, id_partido#2573L, 200)\n                     :     :                                   +- *(9) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n                     :     :                                      +- *(9) SortMergeJoin [id_partido#2573L], [id#2555L], Inner\n                     :     :                                         :- *(6) Sort [id_partido#2573L ASC NULLS FIRST], false, 0\n                     :     :                                         :  +- Exchange hashpartitioning(id_partido#2573L, 200)\n                     :     :                                         :     +- *(5) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n                     :     :                                         :        +- *(5) Filter ((isnotnull(id_partido#2573L) && isnotnull(id_jugador#2571L)) && isnotnull(id_equipo#2572L))\n                     :     :                                         :           +- Scan ExistingRDD[id_jugador#2571L,id_equipo#2572L,id_partido#2573L,puntaje_recibido#2574L,cantidad_amarillas#2575L,cantidad_rojas#2576L,minutos_jugados#2577L]\n                     :     :                                         +- *(8) Sort [id#2555L ASC NULLS FIRST], false, 0\n                     :     :                                            +- Exchange hashpartitioning(id#2555L, 200)\n                     :     :                                               +- *(7) Project [id#2555L]\n                     :     :                                                  +- *(7) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id#2555L))\n                     :     :                                                     +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n                     :     +- *(25) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n                     :        +- Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n                     :           +- *(24) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n                     :              +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n                     +- *(52) Project [id_equipo_visitante#2559L AS id_equipo#3092L, CASE WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END > CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 3 WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END = CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 1 ELSE 0 END AS puntos#3093]\n                        +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n                           :- *(40) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n                           :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n                           :     +- *(39) Project [id#2555L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n                           :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n                           :           :- *(28) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n                           :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n                           :           :     +- *(27) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n                           :           :        +- *(27) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_visitante#2559L))\n                           :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n                           :           +- *(38) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n                           :              +- *(38) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n                           :                 +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n                           +- *(51) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n                              +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2829L], Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat jdk.internal.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange rangepartitioning(sum(puntos)#3103L DESC NULLS LAST, 200)\n+- *(54) HashAggregate(keys=[id_equipo#3087L], functions=[sum(cast(puntos#3088 as bigint))], output=[id_equipo#3087L, sum(puntos)#3103L])\n   +- Exchange hashpartitioning(id_equipo#3087L, 200)\n      +- *(53) HashAggregate(keys=[id_equipo#3087L], functions=[partial_sum(cast(puntos#3088 as bigint))], output=[id_equipo#3087L, sum#3141L])\n         +- Union\n            :- *(26) Project [id_equipo_local#2558L AS id_equipo#3087L, CASE WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END > CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 3 WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END = CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 1 ELSE 0 END AS puntos#3088]\n            :  +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n            :     :- *(14) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n            :     :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n            :     :     +- *(13) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n            :     :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n            :     :           :- *(2) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n            :     :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n            :     :           :     +- *(1) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n            :     :           :        +- *(1) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_local#2558L))\n            :     :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n            :     :           +- *(12) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n            :     :              +- *(12) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n            :     :                 +- Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n            :     :                    +- *(11) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[partial_count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2843L])\n            :     :                       +- *(11) Project [id_equipo#2572L, id_partido#2573L]\n            :     :                          +- *(11) SortMergeJoin [id_jugador#2586L, id_partido_gol#2763L], [id_jugador#2571L, id_partido#2573L], Inner\n            :     :                             :- *(4) Sort [id_jugador#2586L ASC NULLS FIRST, id_partido_gol#2763L ASC NULLS FIRST], false, 0\n            :     :                             :  +- Exchange hashpartitioning(id_jugador#2586L, id_partido_gol#2763L, 200)\n            :     :                             :     +- *(3) Project [id_jugador#2586L, id_partido#2587L AS id_partido_gol#2763L]\n            :     :                             :        +- *(3) Filter (isnotnull(id_partido#2587L) && isnotnull(id_jugador#2586L))\n            :     :                             :           +- Scan ExistingRDD[id#2585L,id_jugador#2586L,id_partido#2587L,tipo_gol#2588,minuto_gol#2589L]\n            :     :                             +- *(10) Sort [id_jugador#2571L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n            :     :                                +- Exchange hashpartitioning(id_jugador#2571L, id_partido#2573L, 200)\n            :     :                                   +- *(9) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n            :     :                                      +- *(9) SortMergeJoin [id_partido#2573L], [id#2555L], Inner\n            :     :                                         :- *(6) Sort [id_partido#2573L ASC NULLS FIRST], false, 0\n            :     :                                         :  +- Exchange hashpartitioning(id_partido#2573L, 200)\n            :     :                                         :     +- *(5) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n            :     :                                         :        +- *(5) Filter ((isnotnull(id_partido#2573L) && isnotnull(id_jugador#2571L)) && isnotnull(id_equipo#2572L))\n            :     :                                         :           +- Scan ExistingRDD[id_jugador#2571L,id_equipo#2572L,id_partido#2573L,puntaje_recibido#2574L,cantidad_amarillas#2575L,cantidad_rojas#2576L,minutos_jugados#2577L]\n            :     :                                         +- *(8) Sort [id#2555L ASC NULLS FIRST], false, 0\n            :     :                                            +- Exchange hashpartitioning(id#2555L, 200)\n            :     :                                               +- *(7) Project [id#2555L]\n            :     :                                                  +- *(7) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id#2555L))\n            :     :                                                     +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n            :     +- *(25) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n            :        +- Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n            :           +- *(24) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n            :              +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n            +- *(52) Project [id_equipo_visitante#2559L AS id_equipo#3092L, CASE WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END > CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 3 WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END = CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 1 ELSE 0 END AS puntos#3093]\n               +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n                  :- *(40) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n                  :     +- *(39) Project [id#2555L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n                  :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n                  :           :- *(28) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n                  :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n                  :           :     +- *(27) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n                  :           :        +- *(27) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_visitante#2559L))\n                  :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n                  :           +- *(38) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n                  :              +- *(38) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n                  :                 +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n                  +- *(51) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n                     +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2829L], Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 57 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(id_equipo#3087L, 200)\n+- *(53) HashAggregate(keys=[id_equipo#3087L], functions=[partial_sum(cast(puntos#3088 as bigint))], output=[id_equipo#3087L, sum#3141L])\n   +- Union\n      :- *(26) Project [id_equipo_local#2558L AS id_equipo#3087L, CASE WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END > CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 3 WHEN (CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END = CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END) THEN 1 ELSE 0 END AS puntos#3088]\n      :  +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n      :     :- *(14) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n      :     :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n      :     :     +- *(13) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n      :     :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n      :     :           :- *(2) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n      :     :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n      :     :           :     +- *(1) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n      :     :           :        +- *(1) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_local#2558L))\n      :     :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n      :     :           +- *(12) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n      :     :              +- *(12) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n      :     :                 +- Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n      :     :                    +- *(11) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[partial_count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2843L])\n      :     :                       +- *(11) Project [id_equipo#2572L, id_partido#2573L]\n      :     :                          +- *(11) SortMergeJoin [id_jugador#2586L, id_partido_gol#2763L], [id_jugador#2571L, id_partido#2573L], Inner\n      :     :                             :- *(4) Sort [id_jugador#2586L ASC NULLS FIRST, id_partido_gol#2763L ASC NULLS FIRST], false, 0\n      :     :                             :  +- Exchange hashpartitioning(id_jugador#2586L, id_partido_gol#2763L, 200)\n      :     :                             :     +- *(3) Project [id_jugador#2586L, id_partido#2587L AS id_partido_gol#2763L]\n      :     :                             :        +- *(3) Filter (isnotnull(id_partido#2587L) && isnotnull(id_jugador#2586L))\n      :     :                             :           +- Scan ExistingRDD[id#2585L,id_jugador#2586L,id_partido#2587L,tipo_gol#2588,minuto_gol#2589L]\n      :     :                             +- *(10) Sort [id_jugador#2571L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n      :     :                                +- Exchange hashpartitioning(id_jugador#2571L, id_partido#2573L, 200)\n      :     :                                   +- *(9) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n      :     :                                      +- *(9) SortMergeJoin [id_partido#2573L], [id#2555L], Inner\n      :     :                                         :- *(6) Sort [id_partido#2573L ASC NULLS FIRST], false, 0\n      :     :                                         :  +- Exchange hashpartitioning(id_partido#2573L, 200)\n      :     :                                         :     +- *(5) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n      :     :                                         :        +- *(5) Filter ((isnotnull(id_partido#2573L) && isnotnull(id_jugador#2571L)) && isnotnull(id_equipo#2572L))\n      :     :                                         :           +- Scan ExistingRDD[id_jugador#2571L,id_equipo#2572L,id_partido#2573L,puntaje_recibido#2574L,cantidad_amarillas#2575L,cantidad_rojas#2576L,minutos_jugados#2577L]\n      :     :                                         +- *(8) Sort [id#2555L ASC NULLS FIRST], false, 0\n      :     :                                            +- Exchange hashpartitioning(id#2555L, 200)\n      :     :                                               +- *(7) Project [id#2555L]\n      :     :                                                  +- *(7) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id#2555L))\n      :     :                                                     +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n      :     +- *(25) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n      :        +- Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n      :           +- *(24) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n      :              +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n      +- *(52) Project [id_equipo_visitante#2559L AS id_equipo#3092L, CASE WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END > CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 3 WHEN (CASE WHEN isnull(count#2829L) THEN 0 ELSE count#2829L END = CASE WHEN isnull(goles_equipo_local#3015L) THEN 0 ELSE goles_equipo_local#3015L END) THEN 1 ELSE 0 END AS puntos#3093]\n         +- SortMergeJoin [id#2555L, id_equipo_visitante#2559L], [id_partido#2573L, id_equipo#2572L], LeftOuter\n            :- *(40) Sort [id#2555L ASC NULLS FIRST, id_equipo_visitante#2559L ASC NULLS FIRST], false, 0\n            :  +- Exchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n            :     +- *(39) Project [id#2555L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n            :        +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n            :           :- *(28) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n            :           :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n            :           :     +- *(27) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n            :           :        +- *(27) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_visitante#2559L))\n            :           :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n            :           +- *(38) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n            :              +- *(38) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n            :                 +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2843L], Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n            +- *(51) Sort [id_partido#2573L ASC NULLS FIRST, id_equipo#2572L ASC NULLS FIRST], false, 0\n               +- ReusedExchange [id_equipo#2572L, id_partido#2573L, count#2829L], Exchange hashpartitioning(id_partido#2573L, id_equipo#2572L, 200)\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 78 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(id#2555L, id_equipo_visitante#2559L, 200)\n+- *(13) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L, count#2829L AS goles_equipo_local#3015L]\n   +- SortMergeJoin [id_equipo_local#2558L, id#2555L], [id_equipo#2572L, id_partido#2573L], LeftOuter\n      :- *(2) Sort [id_equipo_local#2558L ASC NULLS FIRST, id#2555L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id_equipo_local#2558L, id#2555L, 200)\n      :     +- *(1) Project [id#2555L, id_equipo_local#2558L, id_equipo_visitante#2559L]\n      :        +- *(1) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id_equipo_local#2558L))\n      :           +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n      +- *(12) Sort [id_equipo#2572L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n         +- *(12) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2829L])\n            +- Exchange hashpartitioning(id_equipo#2572L, id_partido#2573L, 200)\n               +- *(11) HashAggregate(keys=[id_equipo#2572L, id_partido#2573L], functions=[partial_count(1)], output=[id_equipo#2572L, id_partido#2573L, count#2843L])\n                  +- *(11) Project [id_equipo#2572L, id_partido#2573L]\n                     +- *(11) SortMergeJoin [id_jugador#2586L, id_partido_gol#2763L], [id_jugador#2571L, id_partido#2573L], Inner\n                        :- *(4) Sort [id_jugador#2586L ASC NULLS FIRST, id_partido_gol#2763L ASC NULLS FIRST], false, 0\n                        :  +- Exchange hashpartitioning(id_jugador#2586L, id_partido_gol#2763L, 200)\n                        :     +- *(3) Project [id_jugador#2586L, id_partido#2587L AS id_partido_gol#2763L]\n                        :        +- *(3) Filter (isnotnull(id_partido#2587L) && isnotnull(id_jugador#2586L))\n                        :           +- Scan ExistingRDD[id#2585L,id_jugador#2586L,id_partido#2587L,tipo_gol#2588,minuto_gol#2589L]\n                        +- *(10) Sort [id_jugador#2571L ASC NULLS FIRST, id_partido#2573L ASC NULLS FIRST], false, 0\n                           +- Exchange hashpartitioning(id_jugador#2571L, id_partido#2573L, 200)\n                              +- *(9) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n                                 +- *(9) SortMergeJoin [id_partido#2573L], [id#2555L], Inner\n                                    :- *(6) Sort [id_partido#2573L ASC NULLS FIRST], false, 0\n                                    :  +- Exchange hashpartitioning(id_partido#2573L, 200)\n                                    :     +- *(5) Project [id_jugador#2571L, id_equipo#2572L, id_partido#2573L]\n                                    :        +- *(5) Filter ((isnotnull(id_partido#2573L) && isnotnull(id_jugador#2571L)) && isnotnull(id_equipo#2572L))\n                                    :           +- Scan ExistingRDD[id_jugador#2571L,id_equipo#2572L,id_partido#2573L,puntaje_recibido#2574L,cantidad_amarillas#2575L,cantidad_rojas#2576L,minutos_jugados#2577L]\n                                    +- *(8) Sort [id#2555L ASC NULLS FIRST], false, 0\n                                       +- Exchange hashpartitioning(id#2555L, 200)\n                                          +- *(7) Project [id#2555L]\n                                             +- *(7) Filter ((isnotnull(id_liga#2556L) && (id_liga#2556L = 1)) && isnotnull(id#2555L))\n                                                +- Scan ExistingRDD[id#2555L,id_liga#2556L,fecha#2557,id_equipo_local#2558L,id_equipo_visitante#2559L,estadio#2560,recaudacion#2561,localidades_vendidas#2562L]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:582)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:582)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:582)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 98 more\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1.apply(RDD.scala:886)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1.apply(RDD.scala:886)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.zipPartitions(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$2.apply(RDD.scala:892)\n\tat org.apache.spark.rdd.RDD$$anonfun$zipPartitions$2.apply(RDD.scala:892)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.zipPartitions(RDD.scala:891)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 157 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8E9h_kttAAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Muestro al Campeón:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrWhhbOCtCVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Con first me quedo con el campeon\n",
        "tabla_posiciones.first()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7AMH6huUJj",
        "colab_type": "text"
      },
      "source": [
        "Detengo el Spark Context:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUUhZsmWtHAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFViKSpXPGhL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsYCncgRtPe3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "-- <br/>\n",
        "Juan Manuel Fernandez <br />\n",
        "___Conceptos y Aplicaciones de Big Data - UNLP___"
      ]
    }
  ]
}