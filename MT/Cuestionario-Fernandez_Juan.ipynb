{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuestionario-Fernandez_Juan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/UNLP/blob/master/MT/Cuestionario-Fernandez_Juan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2N7Mww3nsYi",
        "colab_type": "text"
      },
      "source": [
        "# Cuestionario Final: Minería de Textos\n",
        "\n",
        "En esta notebook se responden a las consignas del cuestionario propuesto para aprobar el curso Minería de Textos de la Especialización en Inteligencia de Datos con orientación en Big Data de la Universidad Nacional de La Plata.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "__Estudiante:__ Juan Manuel Fernandez, <br />\n",
        "__DNI:__ 30.939.704.\n",
        "\n",
        "## Parte A: Pre-procesamiento de Textos\n",
        "### Tokenización\n",
        "__1. Dé el vocabulario (y su tamaño) que se generaría cuando la tokenización se realiza por palabra (1-grama), bigrama de palabras y\n",
        "5-gramas de caracteres.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb-0-uASniyI",
        "colab_type": "code",
        "outputId": "de69cdf4-25e1-4326-ef7f-4913a4c5d831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Se define el corpus\n",
        "frase = [\"su auto grande pasó velozmente\", \"tu auto rojo es más caro\"]\n",
        "print(\"frase:{}\\n\".format(frase))\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 1-grama (por palabra)\n",
        "cv = CountVectorizer(ngram_range=(1, 1)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de bigrama (2 palabras)\n",
        "cv = CountVectorizer(ngram_range=(2, 2)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 5-gramas de char (5 char)\n",
        "cv = CountVectorizer(analyzer='char',ngram_range=(5, 5)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frase:['su auto grande pasó velozmente', 'tu auto rojo es más caro']\n",
            "\n",
            "Tamaño de vocabulario: 10\n",
            "Vocabulario:['auto', 'caro', 'es', 'grande', 'más', 'pasó', 'rojo', 'su', 'tu', 'velozmente']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 9\n",
            "Vocabulario:\n",
            "['auto grande', 'auto rojo', 'es más', 'grande pasó', 'más caro', 'pasó velozmente', 'rojo es', 'su auto', 'tu auto']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 43\n",
            "Vocabulario:\n",
            "[' auto', ' caro', ' es m', ' gran', ' más ', ' pasó', ' rojo', ' velo', 'ande ', 'asó v', 'auto ', 'de pa', 'e pas', 'elozm', 'es má', 'grand', 'jo es', 'lozme', 'mente', 'más c', 'nde p', 'o es ', 'o gra', 'o roj', 'ojo e', 'ozmen', 'pasó ', 'rande', 'rojo ', 's car', 's más', 'su au', 'só ve', 'to gr', 'to ro', 'tu au', 'u aut', 'uto g', 'uto r', 'veloz', 'zment', 'ás ca', 'ó vel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmZ9bsMNq83q",
        "colab_type": "text"
      },
      "source": [
        "__2. Dé su opinión de cuales serían las limitaciones de la primera forma de particionado (1-grama) y los posibles patrones que permitiría\n",
        "capturar los restantes dos enfoques.__ \n",
        "\n",
        "Las limitaciones del 1-grama residen en que, por un lado, no puede capturar el contexto de la palabra que si se lograría a través de un n-grama (con n>1) de palabras y en que, por otro lado, no tenemos forma de tomar la \"raíz\" de la palabra sin aplicar alguna técnica adicional (como stemming o lematización), cuestión que si podría lograrse a partir de n-gramas de caracteres.\n",
        "\n",
        "En relación a ésto último, si representamos los términos a partir de n-gramas de caracteres, las palabras con la misma raíz, pero con diferentes sufijos, darían un cierto número de n-gramas iguales más algunos diferentes, es decir, los correspondientes a los sufijos. Esta coincidencia parcial de los n-gramas producidos serviría para establecer una similitud entre ambas palabras, a pesar de que no son exactamente las mismas.\n",
        "Por la misma razón, la extracción de n-gramas puede atenuar los problemas derivados de los errores tipográficos y ortográficos [1].\n",
        "\n",
        "__3. El uso de 3-gramas de caracteres es un enfoque que ha dado buenos\n",
        "resultados en el idioma inglés. ¿Considera usted que en el español podría ser mejor utilizar un número n distinto de caracteres contiguos? Justifique.__  \n",
        "\n",
        "La cantidad de n-gramas de caracteres \"óptima\" puede variar dado que no solo del idioma del texto sino que, además, puede variar dentro del mismo idioma para contextos distintos. No obstante, algunos estudios [2] plantean que el hecho de que, en general, las palabras en el idioma español son mas largas que en el idioma inglés genera que n-gramas mayores a 3 den mejores resultados en nuestro idioma. De hecho, en el estudio citado, los mejores resultados fueron alcanzados con n-gramas con n=6 y n=7.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yFIYgarlJhm",
        "colab_type": "text"
      },
      "source": [
        "### Normalización (truncado y lematización) y POS-tagging\n",
        "\n",
        "__1. Utilizando alguno de los \"stemmers on-line\" provistos en el sitio http://textanalysisonline.com/ (por ejemplo http://textanalysisonline.com/nltk-porter-stemmer), chequee cual sería el resultado que el stemmer de Porter arrojaría para la cadena:\n",
        "_\"Some housewives felt scared when they heard a lot of mice\"___ \n",
        "\n",
        "El resultado de aplicar el stemmer de Porter es _\"Some housew felt scare when they heard a lot of mice\"_. Básicamente, los tokens que modifica son _housewives_ por _housew_ y _scared_ por _scare_.\n",
        "<br /> <br />\n",
        "\n",
        "__2. Describa cual sería el resultado que a su criterio produciría un\n",
        "sistema de lematización con la misma cadena.__ \n",
        "\n",
        "Como vimos durante el curso, la lematización en formas verbales busca el infinitivo y en sustantivos transforma a su forma singular. El lematizador de _spacy_ genera la salida que se puede ver a partir de la próxima celda de código modificando los siguientes términos:\n",
        "- Verbos: felt (feel) y heard (hear),\n",
        "- Sustantivos: housewives (housewive) y mice (mouse).\n",
        "\n",
        "Adicionalmente, transforma a minúsculas el texto y reemplaza el pronombre _they_ por PRON.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v89CUqa3yhnq",
        "colab_type": "code",
        "outputId": "574c635f-feaf-4803-efa5-796d1a1139fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "# cargar el modelo del lenguaje inglés de spacy\n",
        "en_nlp = spacy.load('en')\n",
        "\n",
        "# tokenizar documento con spacy\n",
        "doc_spacy = en_nlp(\"Some housewives felt scared when they heard a lot of mice\")\n",
        "\n",
        "# imprimir lemas encontrados por spacy\n",
        "print(\"Lematización:\")\n",
        "print([token.lemma_ for token in doc_spacy])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lematización:\n",
            "['some', 'housewife', 'feel', 'scared', 'when', '-PRON-', 'hear', 'a', 'lot', 'of', 'mouse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i7YoSMIzLaV",
        "colab_type": "text"
      },
      "source": [
        "__3. Describa y explique cual es el resultado que el POS-tagger online de spaCy (disponible en http://textanalysisonline.com/spacy-pos-tagging) produce con la misma cadena.__  \n",
        "\n",
        "El resultado que arroja el POS-tagger es el siguiente:\n",
        "\"Some|DET housewives|NOUN felt|VERB scared|ADJ when|ADV they|NOUN heard|VERB a|DET lot|NOUN of|ADP mice|NOUN\".\n",
        "\n",
        "El POS-tagger etiqueta los términos del vocabulario de acuerdo al rol que juegan dentro del documento en relación a si se trata de un sujeto, verbo, adjetivo, adverbio, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRa7GZ_TjYNQ",
        "colab_type": "text"
      },
      "source": [
        "### Sentidos de las palabras y Wordnet\n",
        "__1. Defina con sus palabras a qué se denomina synset en Wordnet, y en\n",
        "que consisten las relaciones semánticas de hiperonimia, hiponimia y meronimia.__\n",
        "\n",
        "Un synset es un conjunto de elementos que refieren al mismo tema.\n",
        "A continuación se define el resto de los conceptos:\n",
        "- Hiperonimia: La hiperonimia es una relación que se establece entre una palabra de carácter más general y otra de carácter más específico, es decir, un hiperónimo designa aquel término general que puede ser utilizado para referirse a la realidad nombrada por un término más particular [3].\n",
        "- Hiponimia: Es la relación inversa a la que se produce con las hiperonimias.\n",
        "- Meronimia: Es una relación entre dos palabras donde una refiere a una parte de la otra (pie -> dedo del pié).\n",
        "<br /><br />\n",
        "\n",
        "__2. Descargando y utilizando el sistema Wordnet 2.1 disponible en https://wordnet.princeton.edu/download, determine a que distancia (en número de conceptos recorridos) se encuentra el primer sentido del sustantivo (noun) nose del concepto organ siguiendo la relación de hiperonimia (\"es una clase de\") y a que distancia del concepto de body siguiendo la relación de holonimia (\"es una\n",
        "parte de\"). A modo de ejemplo: si buscamos la palabra cat (gato) y usamos el botón \"Noun\" para ver las distintas relaciones semánticas como sustantivo, veremos que siguiendo la relación de hiperonimia, \"cat\" se encuentra a una distancia de 5 (5 saltos) en la relación de hiperonimia, dada por la siguiente secuencia: cat ⇒ feline ⇒ carnivore ⇒ placental ⇒ mammal ⇒ vertebrate.__\n",
        "\n",
        "<br />\n",
        "Hiperonimia: nose => chemoreceptor => sense organ => organ (3) <br />\n",
        "Holonimia: nose => face => head => body (3)\n",
        "\n",
        "\n",
        "### Reconocimiento de Entidades nombradas\n",
        "\n",
        "__Utlizando el reconocedor de entidades nombradas de spaCy disponible en http://textanalysisonline.com/spacy-named-entity-recognition-ner, describa cual es el resultado del mismo al aplicarlo a las siguientes sentencias: <br/>\n",
        "\"Washinton was born into slavery on the farm of James Burroughs.\" <br/>\n",
        "\"In June, Washington passed a primary seatbelt law.\" <br/>\n",
        "Explique además el porqué del etiquetado diferente de Washington en\n",
        "los dos casos.__  <br/>\n",
        "\n",
        "Para la primera oración el resultado es: <br />\n",
        "- Washinton|PERSON <br />\n",
        "- James Burroughs|PERSON  <br />\n",
        "\n",
        "Mientras que el resultado de la segunda es:  <br />\n",
        "- June|DATE  <br />\n",
        "- Washington|GPE  <br />\n",
        "\n",
        "El resultado es el descripto dado que el reconocedor de entidades describe a Washington como una persona (PERSON) en la primera oración mientras que en la segunda encuentra que se trata de una entidad geopolítica (GPE).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJkt-ynjYlT",
        "colab_type": "text"
      },
      "source": [
        "## Parte B: Representación de documentos\n",
        "\n",
        "### Representación Bolsa de Palabras (BoW)\n",
        "\n",
        "__1. Especifique cómo hubiera sido la representación de vector del primer documento, si en lugar de utilizar un pesado tf, hubiera utilizado una codificación SMART.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xox_EiIrA7jk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c3b544b1-3ab0-4ca4-a9fe-5c29421bd193"
      },
      "source": [
        "documentos = [\"Los jugadores llevaban la pelota oficial al estadio\",\n",
        "              \"Durante el partido, los jugadores no tocaron la pelota\",\n",
        "              \"El estadio de Wembley es el estadio más conocido de Londres\",\n",
        "              \"Los senadores pudieron votar la ley en el congreso\",\n",
        "              \"Todos los senadores del partido opositor pidieron vetar la ley\"]\n",
        "\n",
        "primer_documento = [\"Los jugadores llevaban la pelota oficial al estadio\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect_tf_idf = TfidfVectorizer(use_idf=True, sublinear_tf=False, smooth_idf=False)\n",
        "vect_tf_idf.fit(documentos)\n",
        "\n",
        "bow_tf_idf_1 = vect_tf_idf.transform(primer_documento)\n",
        "print(\"Matriz documentos - términos (pesado tf-idf normalizado):\\n{}\".format(bow_tf_idf_1.toarray()))\n",
        "\n",
        "print(\"\\nTamaño de vocabulario: {}\".format(len(vect_tf_idf.vocabulary_)))\n",
        "\n",
        "print(\"\\nTérminos del vocabulario: {}\".format(vect_tf_idf.get_feature_names()))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz documentos - términos (pesado tf-idf normalizado):\n",
            "[[0.44467176 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.32655323 0.32655323 0.2084347\n",
            "  0.         0.44467176 0.         0.2084347  0.         0.\n",
            "  0.44467176 0.         0.         0.32655323 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]]\n",
            "\n",
            "Tamaño de vocabulario: 30\n",
            "\n",
            "Términos del vocabulario: ['al', 'congreso', 'conocido', 'de', 'del', 'durante', 'el', 'en', 'es', 'estadio', 'jugadores', 'la', 'ley', 'llevaban', 'londres', 'los', 'más', 'no', 'oficial', 'opositor', 'partido', 'pelota', 'pidieron', 'pudieron', 'senadores', 'tocaron', 'todos', 'vetar', 'votar', 'wembley']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97vegtnQD3sN",
        "colab_type": "text"
      },
      "source": [
        "__En las 3 representaciones distribucionales vistas (DOR, TCOR y CSA), primero se obtenía una representación vectorial de los términos y luego se generaba una representación de los documentos haciendo una suma ponderada de los vectores de las palabras que aparecían en cada documento. Esto lleva a que la dimensionalidad de los vectores que representan a los documentos en cada caso, sea igual a la dimensionalidad de los vectores de los términos que le dieron origen. En base a las anteriores consideraciones, diga cual sería la dimensionalidad de un documento en cada uno de los enfoques distribucionales (DOR, TCOR y CSA) para los documentos del ejemplo. Para el caso de CSA, asuma que los documentos están etiquetados como pertenecientes a dos clases: \"deporte\" o \"política\".__ <br />\n",
        "\n",
        "DOR: N (cantidad de documentos).\n",
        "\n",
        "TCOR: M (cantidad de términos).\n",
        "\n",
        "CSA: R (cantidad de clases). En este caso es 2. \n",
        "\n",
        "__Para cada una de las 3 representaciones distribucionales (DOR, TCOR y CSA), obtenga la representación de vector de los términos \"pelota\" y \"partido\". En el caso de CSA, asuma que los 3 primeros documentos están etiquetados como pertenecientes a la clase \"deporte\" y los últimos dos a la clase \"política\".__\n",
        "\n",
        "A continuación se presentan los resultados para la representación DOR:\n",
        "\n",
        "\n",
        "| termino |  d1  |  d2  | d3 | d4 | d5   |\n",
        "| ------- | ---- | ---- | -- | -- | ---- |\n",
        "|pelota   | 0,73 | 0,67 | 0  | 0  | 0    |\n",
        "|partido  | 0    | 0,70 | 0  | 0  | 0,70 |\n",
        "\n",
        "<br />\n",
        "\n",
        "Ahora, se presenta el modelo TCOR:\n",
        "<br /> (Cabe aclarar que se traspone la matríz por el ancho que supone determinar a los términos como columnas, lo cual hace que la visualización se deforme en markdown).\n",
        "\n",
        "|W|pelota|partido|\n",
        "|-|------|-------|\n",
        "|'al'|0,22|-|\n",
        "|'congreso'|-|-|\n",
        "|'conocido'|-|-|\n",
        "|'de'|-|-|\n",
        "|'del'|-|0,22|\n",
        "|'durante'|-|0,22|\n",
        "|'el'|-|0,22|\n",
        "|'en'|-|-|\n",
        "|'es'|-|-|\n",
        "|'estadio'|0,22|-|\n",
        "|'jugadores'|0,28|0,22|\n",
        "|'la'|0,28|0,28|\n",
        "|'ley'|-|0,22|\n",
        "|'llevaban'|0,22|-|\n",
        "|'londres'|-|-|\n",
        "|'los'|0,28|0,28|\n",
        "|'más'|-|-|\n",
        "|'no'|-|0,22|\n",
        "|'oficial'|0,22|-|\n",
        "|'opositor'|-|0,22|\n",
        "|'partido'|-|-|\n",
        "|'pelota'|-|0,22|\n",
        "|'pidieron'|-|0,22|\n",
        "|'pudieron'|-|-|\n",
        "|'senadores'|-|0,22|\n",
        "|'tocaron'|-|0,22|\n",
        "|'todos'|-|0,22|\n",
        "|'vetar'|-|0,22|\n",
        "|'votar'|-|-|\n",
        "|'wembley'|-|-|\n",
        "\n",
        "Por último, se muestra la representación CSA:\n",
        "\n",
        "| termino |  deporte  |  politica  |\n",
        "| ------- |  -------  |  --------  |\n",
        "| pelota  |  2.11     |  0         |\n",
        "| partido |  1.06     |  3.29      |\n",
        "\n",
        "<br />\n",
        "\n",
        "Se puede acceder a una planilla de cálculo con los cálculos auxiliares en la siguiente ubicación:\n",
        "https://github.com/jumafernandez/UNLP/blob/master/MT/Calculos-Auxiliares.xlsx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0bPTeYB99RG",
        "colab_type": "text"
      },
      "source": [
        "## Categorización y clustering de documentos\n",
        "\n",
        "### Evaluación de un clasificador de textos\n",
        "\n",
        "__En base a esta matriz de confusión, se determina:__ <br />\n",
        "__1. El número total de documentos en el test set:__ 100.  <br />\n",
        "__2. El total de documentos que el clasificador categorizó como joven:__ 25. <br />\n",
        "__3. El total de documentos rotulados como adolescente por el experto:__ 57.  <br />\n",
        "__4. La exactitud (accuracy) del clasificador:__ 0.81 (81%).  <br />\n",
        "\n",
        "<br />\n",
        "\n",
        "__Calcule ahora la precision (πC) y recall (ρC) de cada clase C\n",
        "correspondiente a la matriz de confusión del ejercicio anterior.__ <br />\n",
        "\n",
        "| clase        |  recall  |  precision  |\n",
        "| ------------ | -------- | ----------- |\n",
        "|adolescente   | 0,79     |  0,92       |\n",
        "|joven         | 0,76     |  0,64       |\n",
        "|adulto        | 0,91     |  0,77       |\n",
        "\n",
        "<br />\n",
        "\n",
        "__Rara vez precision y recall son consideradas en forma aislada.\n",
        "De ser así, siempre es posible encontrar clasificadores triviales que maximizan una a expensas de la otra. Una solución es encontrar medidas combinadas como la F-measure (medida F).__ \n",
        "<br />\n",
        "__Obtenga la matriz de confusión de cada participante y complete las\n",
        "columnas faltantes, considerando que el ranking oficial se determina\n",
        "por el valor de F0,5.__\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "|Participante|Official rank|Rec|Ac|Precision|Recall|1|0,5|\n",
        "|------------|-------------|---|--|---------|------|-|---|\n",
        "|Tito|2|186|183|0,98|0,72|0,83|0,74|\n",
        "|Pepe|1|204|200|0,98|0,79|0,87|0,80|\n",
        "|Tato|3|181|170|0,94|0,67|0,78|0,69|\n",
        "|Poro|4|159|154|0,97|0,61|0,75|0,63|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivDbxUFgKRsl",
        "colab_type": "text"
      },
      "source": [
        "###  Clustering y medidas de similitud\n",
        "\n",
        "__En base a la información mostrada previamente, calcule la similitud entre los puntos p y q utilizando el coeficiente de matching simple y el coeficiente de Jaccard.__\n",
        "\n",
        "mat-simple = 7/10 = 0.70 <br />\n",
        "\n",
        "Jaccard = 0 <br />\n",
        "\n",
        "__Diga cual es la instancia (documento) más cercana a la instancia i2, de acuerdo al coeficiente de Jaccard.__ <br />\n",
        "\n",
        "i1-i2 = 4/12 = 0.33 <br />\n",
        "i3-i2 = 5/11 = 0.45 <br />\n",
        "i4-i2 = 3/11 = 0.27 <br />\n",
        " <br />\n",
        "\n",
        "La instancia mas cercana es la i3.\n",
        "\n",
        "__Calcule la similitud coseno de los dos objetos de datos p y q que\n",
        "se muestran a continuación, que podrían representar vectores de\n",
        "documentos: <br />\n",
        "p = (3, 2, 0, 5, 0, 0, 0, 2, 0, 0) <br />\n",
        "q = (1, 0, 0, 0, 0, 0, 0, 1, 0, 2)__ <br />\n",
        "\n",
        "cos(p, q) = 5/(6.48*2.44) = 0.31. <br />\n",
        "\n",
        "### Distancia de edición mínima (Levenshtein)\n",
        "\n",
        "__Describa cual sería la distancia de Levenshtein entre las palabras INTENTION y EXECUTION justificando el resultado en base a las operaciones involucradas.__\n",
        "\n",
        "Las operaciones involucradas en la transformación son las siguientes:\n",
        "- Se modifica la I por la E,\n",
        "- Se modifica la N por la X,\n",
        "- Se modifica la T por la E,\n",
        "- Se modifica la E por la C,\n",
        "- Se modifica la N por la U.\n",
        "\n",
        "A partir de lo anterior, se puede deducir que la distancia de Levenshtein es 5.\n",
        "\n",
        "### Evaluación de agrupamientos: Coeficiente de Silueta\n",
        "\n",
        "__Suponga que con los siguientes datos (uni-dimensionales):__  <br />\n",
        "__{1, 5, 9, 20, 25, 30, 35, 52, 54, 58, 60}__, <br />\n",
        "__un algoritmo de clustering ha generado los siguientes grupos:__  <br />\n",
        "__G1 = {1, 5, 9, 20}__ <br />\n",
        "__G2 = {25, 30, 35}__ <br />\n",
        "__G3 = {52, 54, 58, 60}__ <br />  \n",
        "\n",
        "__1. Estime el coeficiente de silueta para el objeto 5 (s(5)) del grupo\n",
        "G1 y para el objeto 20 del mismo grupo.__\n",
        "<br />\n",
        "s(5) = 0.73 <br />\n",
        "s(20) = -0.33 <br />\n",
        "<br />\n",
        "\n",
        "__2. ¿Qué sucede si el objeto 20 es cambiado del grupo G1 al G2? Para\n",
        "determinar eso, recalcule nuevamente los valores de s(5) y s(20)\n",
        "con esta nueva configuración de los grupos.__\n",
        "<br />\n",
        "s(5) = 0.83 <br />\n",
        "s(20) = 0.33 <br />\n",
        "<br />\n",
        "\n",
        "__3. En base a los valores obtenidos en los dos puntos anteriores, ¿qué\n",
        "puede decir sobre la pertenencia o no del objeto 20 al grupo G1?__\n",
        "\n",
        "Claramente, el grupo de pertenencia del punto 20 es el 2 debido a que de esa manera se maximizan las distancias con los demas clusters y se minimiza la intracluster.\n",
        "\n",
        "Se puede acceder a una planilla de cálculo con los cálculos auxiliares en la siguiente ubicación:\n",
        "https://github.com/jumafernandez/UNLP/blob/master/MT/Calculos-Auxiliares.xlsx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5QzrbzQXoJ",
        "colab_type": "text"
      },
      "source": [
        "### Aplicaciones (opcional)\n",
        "__En el artículo _Vector-based word representations for sentiment analysis: a comparative study_ (disponible en http://sedici.unlp.edu.ar/handle/10915/56763) realizamos un estudio preliminar de distintas representaciones de documentos en un problema particular de análisis de sentimiento. Describa cuales fueron las representaciones utilizadas, algoritmos utilizados, resultados obtenidos, etc y dé su opinión sobre este trabajo en base a los conceptos vistos en el curso. Si existe otro tipo de aplicación/trabajo de su interés, puede utilizar ese artículo si lo desea realizando un análisis similar.__ <br /> <br />\n",
        "\n",
        "El objetivo del trabajo consiste en analizar el uso de varias representaciones de palabras para una tarea de análisis de sentimientos sobre críticas de películas con la base de datos de _IMDB Review Dataset_.\n",
        "\n",
        "Específicamente, el trabajo se centra en analizar representaciones basadas en vectores al considerar cuatro métodos del área de distribución (_SOA, LSA, LDA, DOR_) y un representante del enfoque de representación distribuida (_Word2Vec_). A su vez, el estudio toma la representación de _bag of words_ como línea de base.\n",
        "\n",
        "Para la comparación de _performance_ se decidió clasificar las críticas a partir de dos técnicas de clasificación diferentes: _naive Bayes_ y _Lib linear_. \n",
        "Una cuestión importante que contribuye a que el experimento sea reproducible es que constan en el trabajo los parámetros seteados para los mismos.\n",
        "\n",
        "Por último, se decidió evaluar los resultados en función de la métrica de selección de modelos _accuracy_ por encima de otras clásicas -y que trabajamos durante el curso- como _recall, precisión_ o _F-score_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6uda3RRt_fF",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "[1] Pollock, J. J., & Zamora, A. (1984). System design for detection and correction of spelling errors in scientific and scholarly text. Journal of the American Society for Information Science, 35(2), 104-109.\n",
        "\n",
        "[2] Figuerola, C. G., Gómez, R., & de San Román, E. L. (2000). Stemming and n-grams in spanish: an evaluation of their impact on information retrieval. Journal of Information Science, 26(6), 461-467.\n",
        "\n",
        "[3]\tProlee (2019). Hiperonimia. Recuperado de http://www.anep.edu.uy/prolee/index.php/glosario/49-hiperonimia.\n",
        "\n",
        "[4]\tProlee (2019). Hiponimia. Recuperado de http://www.anep.edu.uy/prolee/index.php/glosario/51-hiponimia.\n"
      ]
    }
  ]
}