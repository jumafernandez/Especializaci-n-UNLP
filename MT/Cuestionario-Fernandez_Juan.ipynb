{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuestionario-Minería_de_Textos-UNLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/UNLP/blob/master/MT/Cuestionario-Fernandez_Juan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2N7Mww3nsYi",
        "colab_type": "text"
      },
      "source": [
        "# Cuestionario Final: Minería de Textos\n",
        "\n",
        "En esta notebook se responden a las consignas del cuestionario propuesto para aprobar el curso Minería de Textos de la Especialización en Inteligencia de Datos con orientación en Big Data.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "__Estudiante:__ Juan Manuel Fernandez, <br />\n",
        "__DNI:__ 30.939.704.\n",
        "\n",
        "## Tokenización\n",
        "__1. Dé el vocabulario (y su tamaño) que se generaría cuando la tokenización se realiza por palabra (1-grama), bigrama de palabras y\n",
        "5-gramas de caracteres.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb-0-uASniyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "19b35d29-763d-4906-ce97-7a1d71f201c8"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Se define el corpus\n",
        "frase = [\"su auto grande pasó velozmente\", \"tu auto rojo es más caro\"]\n",
        "print(\"frase:{}\\n\".format(frase))\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 1-grama (por palabra)\n",
        "cv = CountVectorizer(ngram_range=(1, 1)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de bigrama (2 palabras)\n",
        "cv = CountVectorizer(ngram_range=(2, 2)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 5-gramas de char (5 char)\n",
        "cv = CountVectorizer(analyzer='char',ngram_range=(5, 5)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frase:['su auto grande pasó velozmente', 'tu auto rojo es más caro']\n",
            "\n",
            "Tamaño de vocabulario: 10\n",
            "Vocabulario:['auto', 'caro', 'es', 'grande', 'más', 'pasó', 'rojo', 'su', 'tu', 'velozmente']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 9\n",
            "Vocabulario:\n",
            "['auto grande', 'auto rojo', 'es más', 'grande pasó', 'más caro', 'pasó velozmente', 'rojo es', 'su auto', 'tu auto']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 43\n",
            "Vocabulario:\n",
            "[' auto', ' caro', ' es m', ' gran', ' más ', ' pasó', ' rojo', ' velo', 'ande ', 'asó v', 'auto ', 'de pa', 'e pas', 'elozm', 'es má', 'grand', 'jo es', 'lozme', 'mente', 'más c', 'nde p', 'o es ', 'o gra', 'o roj', 'ojo e', 'ozmen', 'pasó ', 'rande', 'rojo ', 's car', 's más', 'su au', 'só ve', 'to gr', 'to ro', 'tu au', 'u aut', 'uto g', 'uto r', 'veloz', 'zment', 'ás ca', 'ó vel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmZ9bsMNq83q",
        "colab_type": "text"
      },
      "source": [
        "__2. Dé su opinión de cuales serían las limitaciones de la primera forma de particionado (1-grama) y los posibles patrones que permitiría\n",
        "capturar los restantes dos enfoques.__ <br />  <br />\n",
        "\n",
        "\n",
        "__3. El uso de 3-gramas de caracteres es un enfoque que ha dado buenos\n",
        "resultados en el idioma inglés. ¿Considera usted que en el español podría ser mejor utilizar un número n distinto de caracteres contiguos? Justifique.__  \n",
        "<br />  <br />\n",
        "\n",
        "##Ejercicio 2 Normalización (truncado y lematización) y POS-tagging\n",
        "\n",
        "__1. Utilizando alguno de los \"stemmers on-line\" provistos en el sitio http://textanalysisonline.com/ (por ejemplo http://textanalysisonline.\n",
        "com/nltk-porter-stemmer), chequee cual sería el resultado que el stemmer de Porter arrojaría para la cadena:\n",
        "_\"Some housewives felt scared when they heard a lot of mice\"___ <br /> <br />\n",
        "\n",
        "__2. Describa cual sería el resultado que a su criterio produciría un\n",
        "sistema de lematización con la misma cadena.__  <br /> <br />\n",
        "\n",
        "__3. Describa y explique cual es el resultado que el POS-tagger online de spaCy (disponible en http://textanalysisonline.com/spacy-pos-tagging) produce con la misma cadena.__  <br /> <br />"
      ]
    }
  ]
}