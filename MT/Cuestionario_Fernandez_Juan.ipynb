{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuestionario-Fernandez_Juan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/UNLP/blob/master/MT/Cuestionario_Fernandez_Juan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2N7Mww3nsYi",
        "colab_type": "text"
      },
      "source": [
        "# Cuestionario Final: Minería de Textos\n",
        "\n",
        "En esta notebook se responden a las consignas del cuestionario propuesto para aprobar el curso Minería de Textos de la Especialización en Inteligencia de Datos con orientación en Big Data de la Universidad Nacional de La Plata.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "__Estudiante:__ Juan Manuel Fernandez, <br />\n",
        "__DNI:__ 30.939.704.\n",
        "\n",
        "## Parte A: Pre-procesamiento de Textos\n",
        "### Tokenización\n",
        "__1. Dé el vocabulario (y su tamaño) que se generaría cuando la tokenización se realiza por palabra (1-grama), bigrama de palabras y\n",
        "5-gramas de caracteres.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb-0-uASniyI",
        "colab_type": "code",
        "outputId": "113ce890-2e46-4c3f-99b8-d78153e8bc50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Se define el corpus\n",
        "frase = [\"su auto grande pasó velozmente\", \"tu auto rojo es más caro\"]\n",
        "print(\"frase:{}\\n\".format(frase))\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 1-grama (por palabra)\n",
        "cv = CountVectorizer(ngram_range=(1, 1)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de bigrama (2 palabras)\n",
        "cv = CountVectorizer(ngram_range=(2, 2)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Se muestra el vocabulario y su tamaño para una tokenización de 5-gramas de char (5 char)\n",
        "cv = CountVectorizer(analyzer='char',ngram_range=(5, 5)).fit(frase)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frase:['su auto grande pasó velozmente', 'tu auto rojo es más caro']\n",
            "\n",
            "Tamaño de vocabulario: 10\n",
            "Vocabulario:['auto', 'caro', 'es', 'grande', 'más', 'pasó', 'rojo', 'su', 'tu', 'velozmente']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 9\n",
            "Vocabulario:\n",
            "['auto grande', 'auto rojo', 'es más', 'grande pasó', 'más caro', 'pasó velozmente', 'rojo es', 'su auto', 'tu auto']\n",
            "\n",
            "\n",
            "Tamaño de vocabulario: 43\n",
            "Vocabulario:\n",
            "[' auto', ' caro', ' es m', ' gran', ' más ', ' pasó', ' rojo', ' velo', 'ande ', 'asó v', 'auto ', 'de pa', 'e pas', 'elozm', 'es má', 'grand', 'jo es', 'lozme', 'mente', 'más c', 'nde p', 'o es ', 'o gra', 'o roj', 'ojo e', 'ozmen', 'pasó ', 'rande', 'rojo ', 's car', 's más', 'su au', 'só ve', 'to gr', 'to ro', 'tu au', 'u aut', 'uto g', 'uto r', 'veloz', 'zment', 'ás ca', 'ó vel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmZ9bsMNq83q",
        "colab_type": "text"
      },
      "source": [
        "__2. Dé su opinión de cuales serían las limitaciones de la primera forma de particionado (1-grama) y los posibles patrones que permitiría\n",
        "capturar los restantes dos enfoques.__ \n",
        "\n",
        "Las limitaciones del 1-grama residen en que, por un lado, no puede capturar el contexto de la palabra que si se lograría a través de un n-grama (con n>1) de palabras y en que, por otro lado, no tenemos forma de tomar la \"raíz\" de la palabra sin aplicar alguna técnica adicional (como stemming o lematización), cuestión que si podría lograrse a partir de n-gramas de caracteres.\n",
        "\n",
        "En relación a ésto último, si representamos los términos a partir de n-gramas de caracteres, las palabras con la misma raíz, pero con diferentes sufijos, darían un cierto número de n-gramas iguales más algunos diferentes, es decir, los correspondientes a los sufijos. Esta coincidencia parcial de los n-gramas producidos serviría para establecer una similitud entre ambas palabras, a pesar de que no son exactamente las mismas.\n",
        "Por la misma razón, la extracción de n-gramas puede atenuar los problemas derivados de los errores tipográficos y ortográficos [1].\n",
        "\n",
        "__3. El uso de 3-gramas de caracteres es un enfoque que ha dado buenos\n",
        "resultados en el idioma inglés. ¿Considera usted que en el español podría ser mejor utilizar un número n distinto de caracteres contiguos? Justifique.__  \n",
        "\n",
        "La cantidad de n-gramas de caracteres \"óptima\" puede variar dado que no solo del idioma del texto sino que, además, puede variar dentro del mismo idioma para contextos distintos. No obstante, algunos estudios [2] plantean que el hecho de que, en general, las palabras en el idioma español son mas largas que en el idioma inglés genera que n-gramas mayores a 3 den mejores resultados en nuestro idioma. De hecho, en el estudio citado, los mejores resultados fueron alcanzados con n-gramas con n=6 y n=7.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yFIYgarlJhm",
        "colab_type": "text"
      },
      "source": [
        "### Normalización (truncado y lematización) y POS-tagging\n",
        "\n",
        "__1. Utilizando alguno de los \"stemmers on-line\" provistos en el sitio http://textanalysisonline.com/ (por ejemplo http://textanalysisonline.com/nltk-porter-stemmer), chequee cual sería el resultado que el stemmer de Porter arrojaría para la cadena:\n",
        "_\"Some housewives felt scared when they heard a lot of mice\"___ \n",
        "\n",
        "El resultado de aplicar el stemmer de Porter es _\"Some housew felt scare when they heard a lot of mice\"_. Básicamente, los tokens que modifica son _housewives_ por _housew_ y _scared_ por _scare_.\n",
        "<br /> <br />\n",
        "\n",
        "__2. Describa cual sería el resultado que a su criterio produciría un\n",
        "sistema de lematización con la misma cadena.__ \n",
        "\n",
        "Como vimos durante el curso, la lematización en formas verbales busca el infinitivo y en sustantivos transforma a su forma singular. El lematizador de _spacy_ genera la salida que se puede ver a partir de la próxima celda de código modificando los siguientes términos:\n",
        "- Verbos: felt (feel) y heard (hear),\n",
        "- Sustantivos: housewives (housewive) y mice (mouse).\n",
        "\n",
        "Adicionalmente, transforma a minúsculas el texto y reemplaza el pronombre _they_ por PRON.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v89CUqa3yhnq",
        "colab_type": "code",
        "outputId": "842c170e-cda5-43ab-aece-aee17084302e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "# cargar el modelo del lenguaje inglés de spacy\n",
        "en_nlp = spacy.load('en')\n",
        "\n",
        "# tokenizar documento con spacy\n",
        "doc_spacy = en_nlp(\"Some housewives felt scared when they heard a lot of mice\")\n",
        "\n",
        "# imprimir lemas encontrados por spacy\n",
        "print(\"Lematización:\")\n",
        "print([token.lemma_ for token in doc_spacy])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lematización:\n",
            "['some', 'housewife', 'feel', 'scared', 'when', '-PRON-', 'hear', 'a', 'lot', 'of', 'mouse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i7YoSMIzLaV",
        "colab_type": "text"
      },
      "source": [
        "__3. Describa y explique cual es el resultado que el POS-tagger online de spaCy (disponible en http://textanalysisonline.com/spacy-pos-tagging) produce con la misma cadena.__  \n",
        "\n",
        "El resultado que arroja el POS-tagger es el siguiente:\n",
        "\"Some|DET housewives|NOUN felt|VERB scared|ADJ when|ADV they|NOUN heard|VERB a|DET lot|NOUN of|ADP mice|NOUN\".\n",
        "\n",
        "El POS-tagger etiqueta los términos del vocabulario de acuerdo al rol que juegan dentro del documento en relación a si se trata de un sujeto, verbo, adjetivo, adverbio, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRa7GZ_TjYNQ",
        "colab_type": "text"
      },
      "source": [
        "### Sentidos de las palabras y Wordnet\n",
        "__1. Defina con sus palabras a qué se denomina synset en Wordnet, y en\n",
        "que consisten las relaciones semánticas de hiperonimia, hiponimia y meronimia.__\n",
        "\n",
        "Un synset es un conjunto de elementos que refieren al mismo tema.\n",
        "A continuación se define el resto de los conceptos:\n",
        "- Hiperonimia: La hiperonimia es una relación que se establece entre una palabra de carácter más general y otra de carácter más específico, es decir, un hiperónimo designa aquel término general que puede ser utilizado para referirse a la realidad nombrada por un término más particular [3].\n",
        "- Hiponimia: Es la relación inversa a la que se produce con las hiperonimias.\n",
        "- Meronimia: Es una relación entre dos palabras donde una refiere a una parte de la otra (pie -> dedo del pié).\n",
        "<br /><br />\n",
        "\n",
        "__2. Descargando y utilizando el sistema Wordnet 2.1 disponible en https://wordnet.princeton.edu/download, determine a que distancia (en número de conceptos recorridos) se encuentra el primer sentido del sustantivo (noun) nose del concepto organ siguiendo la relación de hiperonimia (\"es una clase de\") y a que distancia del concepto de body siguiendo la relación de holonimia (\"es una\n",
        "parte de\"). A modo de ejemplo: si buscamos la palabra cat (gato) y usamos el botón \"Noun\" para ver las distintas relaciones semánticas como sustantivo, veremos que siguiendo la relación de hiperonimia, \"cat\" se encuentra a una distancia de 5 (5 saltos) en la relación de hiperonimia, dada por la siguiente secuencia: cat ⇒ feline ⇒ carnivore ⇒ placental ⇒ mammal ⇒ vertebrate.__\n",
        "\n",
        "<br />\n",
        "Hiperonimia: nose => chemoreceptor => sense organ => organ (3) <br />\n",
        "Holonimia: nose => face => head => body (3)\n",
        "\n",
        "\n",
        "### Reconocimiento de Entidades nombradas\n",
        "\n",
        "__Utlizando el reconocedor de entidades nombradas de spaCy disponible en http://textanalysisonline.com/spacy-named-entity-recognition-ner, describa cual es el resultado del mismo al aplicarlo a las siguientes sentencias: <br/>\n",
        "\"Washinton was born into slavery on the farm of James Burroughs.\" <br/>\n",
        "\"In June, Washington passed a primary seatbelt law.\" <br/>\n",
        "Explique además el porqué del etiquetado diferente de Washington en\n",
        "los dos casos.__  <br/>\n",
        "\n",
        "Para la primera oración el resultado es: <br />\n",
        "- Washinton|PERSON <br />\n",
        "- James Burroughs|PERSON  <br />\n",
        "\n",
        "Mientras que el resultado de la segunda es:  <br />\n",
        "- June|DATE  <br />\n",
        "- Washington|GPE  <br />\n",
        "\n",
        "El resultado es el descripto dado que el reconocedor de entidades describe a Washington como una persona (PERSON) en la primera oración mientras que en la segunda encuentra que se trata de una entidad geopolítica (GPE).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJkt-ynjYlT",
        "colab_type": "text"
      },
      "source": [
        "## Parte B: Representación de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6uda3RRt_fF",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "[1] Pollock, J. J., & Zamora, A. (1984). System design for detection and correction of spelling errors in scientific and scholarly text. Journal of the American Society for Information Science, 35(2), 104-109.\n",
        "\n",
        "[2] Figuerola, C. G., Gómez, R., & de San Román, E. L. (2000). Stemming and n-grams in spanish: an evaluation of their impact on information retrieval. Journal of Information Science, 26(6), 461-467.\n",
        "\n",
        "[3]\tProlee (2019). Hiperonimia. Recuperado de http://www.anep.edu.uy/prolee/index.php/glosario/49-hiperonimia.\n",
        "\n",
        "[4]\tProlee (2019). Hiponimia. Recuperado de http://www.anep.edu.uy/prolee/index.php/glosario/51-hiponimia.\n",
        "\n",
        "[5] \n"
      ]
    }
  ]
}